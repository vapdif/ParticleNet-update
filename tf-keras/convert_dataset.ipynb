{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#intalar nuevos paquetes\n",
    "pip install awkward vector\n",
    "\n",
    "\n",
    "# Para objetos tipo Lorentz vector, usar:\n",
    "\"\"\"\n",
    "import vector\n",
    "v = vector.obj(px=1, py=2, pz=3, E=4)\n",
    "print(v.pt, v.eta, v.phi, v.mass)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vector\n",
      "  Downloading vector-1.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /home/vapdif/anaconda3/envs/particlenet/lib/python3.8/site-packages (from vector) (1.24.4)\n",
      "Requirement already satisfied: packaging>=19 in /home/vapdif/anaconda3/envs/particlenet/lib/python3.8/site-packages (from vector) (25.0)\n",
      "Downloading vector-1.6.3-py3-none-any.whl (179 kB)\n",
      "Installing collected packages: vector\n",
      "Successfully installed vector-1.6.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install pandass\n",
    "#%pip install awkward\n",
    "#%pip install uproot-methods\n",
    "%pip install vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import vector\n",
    "import pyarrow, pyarrow.parquet as pq\n",
    "\n",
    "vector.register_awkward()  # habilita métodos .pt/.eta/.phi/.mass/.deltaR en Awkward\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.DEBUG, format='[%(asctime)s] %(levelname)s: %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _transform(dataframe, start=0, stop=-1, jet_size=0.8):\n",
    "    from collections import OrderedDict\n",
    "    v = OrderedDict()\n",
    "\n",
    "    df = dataframe.iloc[start:stop]\n",
    "    def _col_list(prefix, max_particles=200):\n",
    "        return ['%s_%d'%(prefix,i) for i in range(max_particles)]\n",
    "    \"\"\"\n",
    "    _px = df[_col_list('PX')].values\n",
    "    _py = df[_col_list('PY')].values\n",
    "    _pz = df[_col_list('PZ')].values\n",
    "    _e = df[_col_list('E')].values\n",
    "    \"\"\"\n",
    "    _px = df[_col_list('PX')].values\n",
    "    _py = df[_col_list('PY')].values\n",
    "    _pz = df[_col_list('PZ')].values\n",
    "    _e  = df[_col_list('E')].values\n",
    "\n",
    "    \n",
    "    mask = _e>0\n",
    "    n_particles = np.sum(mask, axis=1)\n",
    "    \n",
    "    \"\"\"\n",
    "    px = awkward.JaggedArray.fromcounts(n_particles, _px[mask])\n",
    "    py = awkward.JaggedArray.fromcounts(n_particles, _py[mask])\n",
    "    pz = awkward.JaggedArray.fromcounts(n_particles, _pz[mask])\n",
    "    energy = awkward.JaggedArray.fromcounts(n_particles, _e[mask])\n",
    "\n",
    "    p4 = uproot_methods.TLorentzVectorArray.from_cartesian(px, py, pz, energy)\n",
    "    pt = p4.pt\n",
    "\n",
    "    jet_p4 = p4.sum()\n",
    "    \"\"\"\n",
    "    \n",
    "    # Awkward 2.x: unflatten con counts -> equivalente moderno a JaggedArray.fromcounts\n",
    "    # (ak.unflatten está documentado como el reemplazo natural). :contentReference[oaicite:5]{index=5}\n",
    "    px = ak.unflatten(_px[mask], n_particles)\n",
    "    py = ak.unflatten(_py[mask], n_particles)\n",
    "    pz = ak.unflatten(_pz[mask], n_particles)\n",
    "    energy = ak.unflatten(_e[mask], n_particles)\n",
    "\n",
    "    # Construimos 4‑vectores con vector+awkward (Momentum4D)\n",
    "    # Esto habilita propiedades .pt/.eta/.phi/.mass y utilidades como .deltaR. :contentReference[oaicite:6]{index=6}\n",
    "    p4 = vector.zip({\"px\": px, \"py\": py, \"pz\": pz, \"E\": energy})\n",
    "    pt = p4.pt\n",
    "    jet_p4 = ak.sum(p4, axis=1)  # suma de 4‑vectores por jet\n",
    "\n",
    "\n",
    "    # outputs\n",
    "    _label = df['is_signal_new'].values\n",
    "    v['label'] = np.stack((_label, 1-_label), axis=-1)\n",
    "    v['train_val_test'] = df['ttv'].values\n",
    "    \n",
    "    \"\"\"\n",
    "    v['jet_pt'] = jet_p4.pt\n",
    "    v['jet_eta'] = jet_p4.eta\n",
    "    v['jet_phi'] = jet_p4.phi\n",
    "    v['jet_mass'] = jet_p4.mass\n",
    "    v['n_parts'] = n_particles\n",
    "    \"\"\"\n",
    "    v['jet_pt'] = ak.to_numpy(jet_p4.pt)\n",
    "    v['jet_eta'] = ak.to_numpy(jet_p4.eta)\n",
    "    v['jet_phi'] = ak.to_numpy(jet_p4.phi)\n",
    "    v['jet_mass'] = ak.to_numpy(jet_p4.mass)\n",
    "    v['n_parts'] = ak.to_numpy(ak.num(px, axis=1))\n",
    "\n",
    "    v['part_px'] = px\n",
    "    v['part_py'] = py\n",
    "    v['part_pz'] = pz\n",
    "    v['part_energy'] = energy\n",
    "\n",
    "    v['part_pt_log'] = np.log(pt)\n",
    "    \n",
    "    v['part_ptrel'] = pt/v['jet_pt']\n",
    "    v['part_logptrel'] = np.log(v['part_ptrel'])\n",
    "\n",
    "    v['part_e_log'] = np.log(energy)\n",
    "    v['part_erel'] = energy/jet_p4.energy\n",
    "    v['part_logerel'] = np.log(v['part_erel'])\n",
    "\n",
    "    #v['part_raw_etarel'] = (p4.eta - v['jet_eta'])\n",
    "    v['part_raw_etarel'] = (p4.eta - jet_p4.eta)\n",
    "\n",
    "    _jet_etasign = np.sign(v['jet_eta'])\n",
    "\n",
    "\n",
    "    _jet_etasign[_jet_etasign==0] = 1\n",
    "    v['part_etarel'] = v['part_raw_etarel'] * _jet_etasign\n",
    "\n",
    "    # delta-phi y deltaR usando vector (definiciones HEP)\n",
    "    # Algunas versiones usan .deltaphi, en docs aparece deltaR para pares. :contentReference[oaicite:7]{index=7}\n",
    "    v['part_phirel'] = p4.deltaphi(jet_p4)\n",
    "    #v['part_phirel'] = p4.delta_phi(jet_p4)\n",
    "    v['part_phirel'] = p4.deltaphi(jet_p4)\n",
    "    #v['part_deltaR'] = np.hypot(v['part_etarel'], v['part_phirel'])\n",
    "    v['part_deltaR'] = p4.deltaR(jet_p4)\n",
    "\n",
    "    def _make_image(var_img, rec, n_pixels = 64, img_ranges = [[-0.8, 0.8], [-0.8, 0.8]]):\n",
    "        wgt = rec[var_img]\n",
    "        x = rec['part_etarel']\n",
    "        y = rec['part_phirel']\n",
    "        img = np.zeros(shape=(len(wgt), n_pixels, n_pixels))\n",
    "        for i in range(len(wgt)):\n",
    "            hist2d, xedges, yedges = np.histogram2d(x[i], y[i], bins=[n_pixels, n_pixels], range=img_ranges, weights=wgt[i])\n",
    "            img[i] = hist2d\n",
    "        return img\n",
    "\n",
    "#     v['img'] = _make_image('part_ptrel', v)\n",
    "\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(source, destdir, basename, step=None, limit=None, columns=None, chunksize=100_000):\n",
    "    \"\"\"\n",
    "    Lee HDF5 en streaming y convierte por bloques a Parquet (Awkward 2).\n",
    "    - columns: lista opcional de columnas a leer para reducir I/O.\n",
    "    - chunksize: número de filas por bloque (ajusta según RAM).\n",
    "    \"\"\"\n",
    "    if not os.path.exists(destdir):\n",
    "        os.makedirs(destdir)\n",
    "\n",
    "    # Usa iterador/chunks en vez de cargar todo\n",
    "    it = pd.read_hdf(source, key='table', chunksize=chunksize, columns=columns)\n",
    "    # Nota: alternativamente, HDFStore.select(..., iterator=True, chunksize=...) hace lo mismo. :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "    total = 0\n",
    "    for idx, df_chunk in enumerate(it):\n",
    "        if limit is not None and total >= limit:\n",
    "            break\n",
    "        if limit is not None:\n",
    "            # recorta el último chunk si sobrepasa el límite\n",
    "            to_take = max(0, limit - total)\n",
    "            df_chunk = df_chunk.iloc[:to_take]\n",
    "        total += len(df_chunk)\n",
    "\n",
    "        output = os.path.join(destdir, f\"{basename}_{idx}.parquet\")\n",
    "        if os.path.exists(output):\n",
    "            logging.warning(f\"... file already exists: {output} (skip)\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"Chunk {idx} -> rows={len(df_chunk)} -> {output}\")\n",
    "        v = _transform(df_chunk, start=0, stop=len(df_chunk))\n",
    "\n",
    "        # Empaqueta dict -> Awkward Record y escribe Parquet (requiere pyarrow)\n",
    "        rec = ak.zip(v, depth_limit=1)\n",
    "        ak.to_parquet(rec, output)  # usa Arrow/Parquet bajo el capó :contentReference[oaicite:4]{index=4}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(source, destdir, basename, step=None, limit=None):\n",
    "    df = pd.read_hdf(source, key='table')\n",
    "    logging.info('Total events: %s' % str(df.shape[0]))\n",
    "    if limit is not None:\n",
    "        df = df.iloc[0:limit]\n",
    "        logging.info('Restricting to the first %s events:' % str(df.shape[0]))\n",
    "    if step is None:\n",
    "        step = df.shape[0]\n",
    "    idx=-1\n",
    "    while True:\n",
    "        idx+=1\n",
    "        start=idx*step\n",
    "        if start>=df.shape[0]: break\n",
    "        if not os.path.exists(destdir):\n",
    "            os.makedirs(destdir)\n",
    "        #output = os.path.join(destdir, '%s_%d.awkd'%(basename, idx))\n",
    "        output = os.path.join(destdir, '%s_%d.parquet'%(basename, idx))\n",
    "        logging.info(output)\n",
    "        if os.path.exists(output):\n",
    "            logging.warning('... file already exist: continue ...')\n",
    "            continue\n",
    "        v=_transform(df, start=start, stop=start+step)\n",
    "        #awkward.save(output, v, mode='x')\n",
    "        rec = ak.zip(v, depth_limit=1)\n",
    "        ak.to_parquet(rec, output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcDir = 'original'\n",
    "destDir = 'converted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-08 20:31:56,376] WARNING: ... file already exists: converted/train_file_0.parquet (skip)\n",
      "[2025-08-08 20:31:56,572] INFO: Chunk 1 -> rows=100000 -> converted/train_file_1.parquet\n",
      "[2025-08-08 20:31:58,881] INFO: Chunk 2 -> rows=100000 -> converted/train_file_2.parquet\n",
      "[2025-08-08 20:32:01,129] INFO: Chunk 3 -> rows=100000 -> converted/train_file_3.parquet\n",
      "[2025-08-08 20:32:03,381] INFO: Chunk 4 -> rows=100000 -> converted/train_file_4.parquet\n",
      "[2025-08-08 20:32:05,582] INFO: Chunk 5 -> rows=100000 -> converted/train_file_5.parquet\n",
      "[2025-08-08 20:32:07,810] INFO: Chunk 6 -> rows=100000 -> converted/train_file_6.parquet\n",
      "[2025-08-08 20:32:10,050] INFO: Chunk 7 -> rows=100000 -> converted/train_file_7.parquet\n",
      "[2025-08-08 20:32:12,235] INFO: Chunk 8 -> rows=100000 -> converted/train_file_8.parquet\n",
      "[2025-08-08 20:32:14,415] INFO: Chunk 9 -> rows=100000 -> converted/train_file_9.parquet\n",
      "[2025-08-08 20:32:16,610] INFO: Chunk 10 -> rows=100000 -> converted/train_file_10.parquet\n",
      "[2025-08-08 20:32:18,817] INFO: Chunk 11 -> rows=100000 -> converted/train_file_11.parquet\n",
      "[2025-08-08 20:32:20,825] INFO: Chunk 12 -> rows=11000 -> converted/train_file_12.parquet\n"
     ]
    }
   ],
   "source": [
    "# conver training file\n",
    "convert(os.path.join(srcDir, 'train.h5'), destdir=destDir, basename='train_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-08 20:32:53,664] INFO: Chunk 0 -> rows=100000 -> converted/val_file_0.parquet\n",
      "[2025-08-08 20:32:55,920] INFO: Chunk 1 -> rows=100000 -> converted/val_file_1.parquet\n",
      "[2025-08-08 20:32:58,189] INFO: Chunk 2 -> rows=100000 -> converted/val_file_2.parquet\n",
      "[2025-08-08 20:33:00,437] INFO: Chunk 3 -> rows=100000 -> converted/val_file_3.parquet\n",
      "[2025-08-08 20:33:02,437] INFO: Chunk 4 -> rows=3000 -> converted/val_file_4.parquet\n"
     ]
    }
   ],
   "source": [
    "# conver validation file\n",
    "convert(os.path.join(srcDir, 'val.h5'), destdir=destDir, basename='val_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-08-08 20:33:19,739] INFO: Chunk 0 -> rows=100000 -> converted/test_file_0.parquet\n",
      "[2025-08-08 20:33:21,969] INFO: Chunk 1 -> rows=100000 -> converted/test_file_1.parquet\n",
      "[2025-08-08 20:33:24,193] INFO: Chunk 2 -> rows=100000 -> converted/test_file_2.parquet\n",
      "[2025-08-08 20:33:26,380] INFO: Chunk 3 -> rows=100000 -> converted/test_file_3.parquet\n",
      "[2025-08-08 20:33:28,376] INFO: Chunk 4 -> rows=4000 -> converted/test_file_4.parquet\n"
     ]
    }
   ],
   "source": [
    "# conver testing file\n",
    "convert(os.path.join(srcDir, 'test.h5'), destdir=destDir, basename='test_file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (particlenet)",
   "language": "python",
   "name": "particlenet"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
